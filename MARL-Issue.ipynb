{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c123a57b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T13:39:31.709910Z",
     "start_time": "2022-01-04T13:39:31.688983Z"
    }
   },
   "outputs": [],
   "source": [
    "from ray.rllib.env import MultiAgentEnv\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "import ray\n",
    "from ray import tune\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d477e8c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T13:39:31.725865Z",
     "start_time": "2022-01-04T13:39:31.712900Z"
    }
   },
   "outputs": [],
   "source": [
    "class IrrigationEnv(MultiAgentEnv):\n",
    "    def __init__(self, return_agent_actions = False, part=False):\n",
    "        self.num_agents = 5\n",
    "        self.observation_space = gym.spaces.Box(low=200, high=800, shape=(1,))\n",
    "        self.action_space = gym.spaces.Box(low=0, high=1, shape=(1,))\n",
    "\n",
    "    def reset(self):\n",
    "        obs = {}\n",
    "        self.water = np.random.uniform(200,800)\n",
    "        for i in range(self.num_agents):\n",
    "            obs[i] = np.array([self.water])\n",
    "        return obs\n",
    "\n",
    "    def cal_rewards(self, action_dict):\n",
    "        self.curr_water = self.water\n",
    "        reward = 0\n",
    "        for i in range(self.num_agents):\n",
    "            water_demanded = self.water*action_dict[i][0]\n",
    "            if self.curr_water == 0:\n",
    "                # No water is left in stream\n",
    "                reward -= water_demanded*100 # Penalty\n",
    "            elif self.curr_water - water_demanded<0:\n",
    "                # Water in stream is less than water demanded, withdraw all left\n",
    "                water_needed = water_demanded - self.curr_water\n",
    "                water_withdrawn = self.curr_water\n",
    "                self.curr_water = 0\n",
    "                reward += -water_withdrawn**2 + 200*water_withdrawn\n",
    "                reward -= water_needed*100 # Penalty\n",
    "            else:\n",
    "                # Water in stream is more than water demanded, withdraw water demanded\n",
    "                self.curr_water -= water_demanded\n",
    "                water_withdrawn = water_demanded\n",
    "                reward += -water_withdrawn**2 + 200*water_withdrawn\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        obs, rew, done, info = {}, {}, {}, {}\n",
    "\n",
    "        reward = self.cal_rewards(action_dict)\n",
    "\n",
    "        for i in range(self.num_agents):\n",
    "\n",
    "            obs[i], rew[i], done[i], info[i] = np.array([self.curr_water]), reward, True, {}\n",
    "\n",
    "        done[\"__all__\"] = True\n",
    "        return obs, rew, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df9d281d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T13:40:13.277672Z",
     "start_time": "2022-01-04T13:39:31.728858Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " pid=2692)\u001b[0m 2022-01-04 14:39:43,740\tWARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "2022-01-04 14:39:53,709\tINFO trainable.py:124 -- Trainable.setup took 21.946 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-01-04 14:39:53,714\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; avg. reward=-779603.5833117398\n",
      "Iter: 1; avg. reward=-515774.1297232858\n",
      "Iter: 2; avg. reward=-318073.5645450128\n",
      "Iter: 3; avg. reward=-217518.62759570783\n",
      "Iter: 4; avg. reward=-80784.4438319313\n",
      "Iter: 5; avg. reward=-18121.005286384003\n",
      "Iter: 6; avg. reward=-2725.425218754961\n",
      "Iter: 7; avg. reward=45630.70374515159\n",
      "Iter: 8; avg. reward=57204.84265795139\n",
      "Iter: 9; avg. reward=69085.41056414637\n",
      "Iter: 10; avg. reward=74416.85820282507\n",
      "Iter: 11; avg. reward=98381.41331161988\n",
      "Iter: 12; avg. reward=118006.33517910565\n",
      "Iter: 13; avg. reward=136276.57466463174\n",
      "Iter: 14; avg. reward=147511.4776076984\n"
     ]
    }
   ],
   "source": [
    "def env_creator(_):\n",
    "    return IrrigationEnv()\n",
    "\n",
    "single_env = IrrigationEnv()\n",
    "env_name = \"IrrigationEnv\"\n",
    "register_env(env_name, env_creator)\n",
    "\n",
    "# Get environment obs, action spaces and number of agents\n",
    "obs_space = single_env.observation_space\n",
    "act_space = single_env.action_space\n",
    "num_agents = single_env.num_agents\n",
    "\n",
    "# Create a policy mapping\n",
    "def gen_policy():\n",
    "    return (None, obs_space, act_space, {})\n",
    "\n",
    "policy_graphs = {}\n",
    "for i in range(num_agents):\n",
    "    policy_graphs['agent-' + str(i)] = gen_policy()\n",
    "\n",
    "#Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead\n",
    "def policy_mapping_fn(agent_id, episode, worker):\n",
    "    return 'agent-' + str(agent_id)\n",
    "\n",
    "config={\n",
    "            \"log_level\": \"WARN\",\n",
    "            \"num_workers\": 1,\n",
    "            \"num_cpus_for_driver\": 1,\n",
    "            \"num_cpus_per_worker\": 1,\n",
    "            \"num_sgd_iter\": 10,\n",
    "            \"train_batch_size\": 128,\n",
    "            \"lr\": 5e-3,\n",
    "            \"model\":{\"fcnet_hiddens\": [8, 8]},\n",
    "            \"rollout_fragment_length\": 128,\n",
    "            \"multiagent\": {\n",
    "                \"policies\": policy_graphs,\n",
    "                \"policy_mapping_fn\": policy_mapping_fn,\n",
    "            },\n",
    "            \"env\": \"IrrigationEnv\"}\n",
    "    \n",
    "trainer = PPOTrainer(config=config)\n",
    "\n",
    "for i in range(15):\n",
    "    results = trainer.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8a748bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T13:40:13.293665Z",
     "start_time": "2022-01-04T13:40:13.282662Z"
    }
   },
   "outputs": [],
   "source": [
    "env = IrrigationEnv()\n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b6493fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T13:40:13.544951Z",
     "start_time": "2022-01-04T13:40:13.298616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-reward=-5394731.978702811\n"
     ]
    }
   ],
   "source": [
    "while not done:\n",
    "   \n",
    "    action = {}\n",
    "    for agent_id, agent_obs in obs.items():\n",
    "        policy_id = config['multiagent']['policy_mapping_fn'](agent_id, None, None)\n",
    "        action[agent_id] = trainer.compute_single_action(agent_obs, policy_id=policy_id)\n",
    "        \n",
    "    obs, reward, done, info = env.step(action)\n",
    "    done = done['__all__']\n",
    "    \n",
    "    total_reward += sum(reward.values())\n",
    "\n",
    "print(f\"total-reward={total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead479d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
